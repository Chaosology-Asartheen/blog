<!DOCTYPE html>





<html lang="zh-TW">
<head>
  <meta name="google-site-verification" content="exmdvS3MT1hVojBxSEZZZ8TIBBOg6pJ2POTtzNuecqs" />
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/blog/images/logo.svg?v=7.4.0" color="#222">

<link rel="stylesheet" href="/blog/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Gemini',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","width":280,"display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '複製',
      copy_success: '複製成功',
      copy_failure: '複製失敗'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="本文為一系列課程之筆記，建議從” 機器學習基石筆記-1 “開始閱讀&amp;gt; 本文討論內容請參考:機器學習基石第九講 : Linear Regression機器學習基石第十講 : Logistic Regression  本篇所有圖片部分由筆者製作，其它均為機器學習基石課程內容講義">
<meta property="og:type" content="article">
<meta property="og:title" content="林軒田機器學習基石筆記 - 第九講、第十講">
<meta property="og:url" content="https://allen108108.github.io/blog/2019/10/07/林軒田機器學習基石筆記 - 第九講、第十講/index.html">
<meta property="og:site_name" content="Math.py">
<meta property="og:description" content="本文為一系列課程之筆記，建議從” 機器學習基石筆記-1 “開始閱讀&amp;gt; 本文討論內容請參考:機器學習基石第九講 : Linear Regression機器學習基石第十講 : Logistic Regression  本篇所有圖片部分由筆者製作，其它均為機器學習基石課程內容講義">
<meta property="og:locale" content="zh-TW">
<meta property="og:image" content="https://i.imgur.com/4RQIFTU.png">
<meta property="og:image" content="https://i.imgur.com/9Ff7ClN.png">
<meta property="og:image" content="https://i.imgur.com/4nkAl7H.png">
<meta property="og:image" content="https://i.imgur.com/PuLGZVT.png">
<meta property="og:image" content="https://i.imgur.com/kr4qkyR.png">
<meta property="og:image" content="https://i.imgur.com/N4eRcKO.png">
<meta property="og:image" content="https://i.imgur.com/LqMNoeq.png">
<meta property="og:image" content="https://i.imgur.com/2Ce4cew.png">
<meta property="og:image" content="https://i.imgur.com/30cDMOj.png =290x">
<meta property="og:image" content="https://i.imgur.com/J70UyQU.png =280x">
<meta property="og:updated_time" content="2019-10-07T05:27:33.532Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="林軒田機器學習基石筆記 - 第九講、第十講">
<meta name="twitter:description" content="本文為一系列課程之筆記，建議從” 機器學習基石筆記-1 “開始閱讀&amp;gt; 本文討論內容請參考:機器學習基石第九講 : Linear Regression機器學習基石第十講 : Logistic Regression  本篇所有圖片部分由筆者製作，其它均為機器學習基石課程內容講義">
<meta name="twitter:image" content="https://i.imgur.com/4RQIFTU.png">
  <link rel="canonical" href="https://allen108108.github.io/blog/2019/10/07/林軒田機器學習基石筆記 - 第九講、第十講/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>林軒田機器學習基石筆記 - 第九講、第十講 | Math.py</title>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149442581-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-149442581-1');
    }
  </script>








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-TW">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/blog/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Math.py</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Wir müssen wissen , wir werden wissen</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切換導航欄">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/blog/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首頁</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/blog/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分類</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/blog/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>歸檔</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://allen108108.github.io/blog/blog/2019/10/07/林軒田機器學習基石筆記 - 第九講、第十講/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Allen Tzeng">
      <meta itemprop="description" content="Study about Mathematics , Programming and Data Science">
      <meta itemprop="image" content="/blog/images/allen.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Math.py">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">林軒田機器學習基石筆記 - 第九講、第十講

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              
                
              

              <time title="創建時間：2019-10-07 13:19:26 / 修改時間：13:27:33" itemprop="dateCreated datePublished" datetime="2019-10-07T13:19:26+08:00">2019-10-07</time>
            </span>
          
            

            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/林軒田-機器學習基石/" itemprop="url" rel="index"><span itemprop="name">林軒田 機器學習基石</span></a></span>

                
                
              
            </span>
          

          
            <span class="post-meta-item" title="閱讀次數" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<ul>
<li><strong>本文為一系列課程之筆記，建議從” <a href="https://hackmd.io/s/ryxzB7LwN" target="_blank" rel="noopener">機器學習基石筆記-1</a> “開始閱讀</strong><br>&gt;</li>
<li><p><strong>本文討論內容請參考:<br>機器學習基石第九講 : Linear Regression</strong><br><strong>機器學習基石第十講 : Logistic Regression</strong></p>
</li>
<li><p><strong>本篇所有圖片部分由筆者製作，其它均為機器學習基石課程內容講義</strong></p>
</li>
</ul>
<hr>
</blockquote>
<a id="more"></a>
<p>在開始接下來的課程前，我想先緩一緩腳步。</p>
<p>從前面的課程一路到現在，我們學到了這麼多的想法、概念、工具，究竟這些東西的關聯性是什麼 ? 它們跟機器學習的關係又是什麼 ?</p>
<p>當我們手中有一堆資料 ( $\mathbb{D}$ )，我們會先問自己，我們想要從這些資料裡面知道什麼 ? 預測什麼 ? ( $Classification\ or\ Regression$ ) 在這樣的問題下，我們最終會希望有一個模型 ( $g$ ) 可以盡量準確地預測出我們想要知道的事情。( $g\approx f$ )</p>
<p>想要得到這樣的模型，我們就得從許多可能性中 ( $h$ )，找出跟我們手中資料真實狀況誤差最小的 ( $\min E_{in}(h)=\min err(y,\hat y)$ ) 來作為我們的模型。而這樣找出最小值的過程，就是我們所謂演算法 ( $algorithm$ )。</p>
<p>PS:<br>當我們決定了想要什麼樣子型態的模型 ( PLA , Pocket , Linear regression , Logistic regression…) ，那麼找出最小值的演算法也同時被確定下來。</p>
<p>當我們理清楚其中的關係後，很清楚地可以知道，<font color="#dd0000">所有演算法就是針對某一個error measure 尋找最小值的方法</font>，也就是說，當我們要決定模型的型態，也要先決定出來我們要用什麼樣子的 error measure 。( 回想一下，當我們決定了 error measure 後，機器學習的可行性便也可以確定<sup><a href="#fn_註1" id="reffn_註1">註1</a></sup>)</p>
<h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p><img src="https://i.imgur.com/4RQIFTU.png" alt><br>( 紅線部分我們稱為 剩餘residuals )</p>
<p>Linear regression : 找出 line / hyperplane 有最小的 residuals</p>
<h3 id="Linear-Regression-Algorithm"><a href="#Linear-Regression-Algorithm" class="headerlink" title="Linear Regression Algorithm"></a>Linear Regression Algorithm</h3><p>在線性回歸中，最常用的 error measure 就是 square error</p>
<p>$err(y,\hat y)=(y-\hat y)^2$<br>$\Longrightarrow E_{in}(h)=\frac{1}{N}\sum\limits_{n=1}^{N}(h(\mathbb{X}_n)-y_n)^2\ and\ E_{out}(h)=\underset{(\mathbb{X},y)\sim\mathbb{P}}{\mathbb{E}}(\mathbb{W}^T\mathbb{X}-y)^2$</p>
<p><img src="https://i.imgur.com/9Ff7ClN.png" alt></p>
<p>Step 1 : 先將資料整理成矩陣型態，找出以每一筆資料為列向量的 $\mathbb{X}$ 矩陣及 $\mathbb{Y}$ 矩陣</p>
<p>Step 2 : 計算出 pseudo-inverse<sup><a href="#fn_註2" id="reffn_註2">註2</a></sup><sup><a href="#fn_註3" id="reffn_註3">註3</a></sup> $\mathbb{X}^\dagger=\begin{cases}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T,&amp; \mbox{if}\ \mathbb{X}^T\mathbb{X}\ \mbox{is invertible}\\defined\ by\ other\ way,&amp;\mbox{if}\ \mathbb{X}^T\mathbb{X}\ \mbox{is’nt invertible}\end{cases}$</p>
<p>Step 3 : 最佳解 $\mathbb{W}_{lin}=\mathbb{X}^\dagger\mathbb{Y}$</p>
<hr>
<p>$Proof$</p>
<p>(1) $Existence :$</p>
<p>$E_{in}(\mathbb{W})=\frac{1}{N}\sum\limits_{n=1}^{N}(\mathbb{W}^T\mathbb{X}_n-y_n)^2=\frac{1}{N}\sum\limits_{n=1}^{N}({\mathbb{X}_n}^T\mathbb{W}-y_n)^2$</p>
<p>$=\frac{1}{N}\begin{Vmatrix}<br>{\mathbb{X}_1}^T\mathbb{W}-y_1  \\<br>{\mathbb{X}_2}^T\mathbb{W}-y_2  \\<br>\vdots\\<br>{\mathbb{X}_N}^T\mathbb{W}-y_N  \\ \end{Vmatrix}^2=\frac{1}{N}\begin{Vmatrix}\begin{bmatrix}{\mathbb{X}_1}^T \\{\mathbb{X}_2}^T\\ \vdots \\{\mathbb{X}_N}^T\end{bmatrix}\mathbb{W}-\begin{bmatrix}y_1\\y_2\\ \vdots \\y_N\end{bmatrix}<br>\end{Vmatrix}^2=\frac{1}{N}\begin{Vmatrix}\mathbb{X}\mathbb{W}-\mathbb{Y}\end{Vmatrix}^2$</p>
<p>$\because E_{in}(\mathbb{W})$ is a conti , diff ,convex function</p>
<p>$\therefore \exists \mathbb{W}_{lin}$ s.t. $\nabla E_{in}(\mathbb{W}_{lin})=\begin{bmatrix}\frac{\partial E_{in}}{\partial w_0}(\mathbb{W}_{lin})\\\frac{\partial E_{in}}{\partial w_1}(\mathbb{W}_{lin})\\ \vdots\\\frac{\partial E_{in}}{\partial w_d}(\mathbb{W}_{lin})\\\end{bmatrix}=\begin{bmatrix}0\\0\\\vdots\\0\end{bmatrix}$</p>
<p>(2)</p>
<p>$\because E_{in}(\mathbb{W})=\frac{1}{N}\begin{Vmatrix}\mathbb{X}\mathbb{W}-\mathbb{Y}\end{Vmatrix}^2=\frac{1}{N}(\mathbb{W}^T\mathbb{X}^T\mathbb{X}\mathbb{W}-2\mathbb{W}^T\mathbb{X}^T\mathbb{Y}+\mathbb{Y}^T\mathbb{Y})$</p>
<p>$\overset{let}{=}\frac{1}{N}(\mathbb{W}^T\mathbb{A}\mathbb{W}-2\mathbb{W}^T\mathbb{B}+\mathbb{C})$</p>
<p>$\Longrightarrow \nabla E_{in}(\mathbb{W})=\frac{1}{N}(2\mathbb{A}\mathbb{W}-2\mathbb{B})=\frac{2}{N}(\mathbb{X}^T\mathbb{X}\mathbb{W}-\mathbb{X}^T\mathbb{Y})\overset{let}{=}0$</p>
<p>$\Longrightarrow\mathbb{W}=\mathbb{W}_{lin}=\mathbb{X}^\dagger\mathbb{Y}\ where\ \mathbb{X}^\dagger=\begin{cases}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T,&amp; \mbox{if}\ \mathbb{X}^T\mathbb{X}\ \mbox{is invertible}\\defined\ by\ other\ way,&amp;\mbox{if}\ \mathbb{X}^T\mathbb{X}\ \mbox{is’nt invertible}\end{cases}$</p>
<h3 id="Linear-Regression-的可行性"><a href="#Linear-Regression-的可行性" class="headerlink" title="Linear Regression 的可行性"></a>Linear Regression 的可行性</h3><p>由上述我們可以找出有最小誤差的預測模型 $\hat{\mathbb{Y}}=\mathbb{X}\mathbb{W}_{lin}=\mathbb{X}(\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}\overset{let}{=}\mathbb{H}\mathbb{Y}$</p>
<p>我們可以把這個 $\mathbb{H}$ 看作是一種線性變換，將 $\mathbb{Y}$ 轉換到 $\hat{\mathbb{Y}}$ ，再更白話一點說，$\mathbb{H}$ 就是 $\mathbb{Y}$ 與 $\hat{\mathbb{Y}}$ 的一個線性關係。而這樣的一個關係它有一些特別的特性 : </p>
<ol>
<li>$\mathbb{H}$ is symmetric</li>
<li>$\mathbb{H}^k=\mathbb{H}$</li>
<li>$(\mathbb{I}-\mathbb{H})^k=\mathbb{I}-\mathbb{H}$</li>
<li>$tr(\mathbb{H})=d+1$</li>
</ol>
<p>我們可以利用這些特性推導出<br>$\overline{E_{in}}=\underset{\mathbb{D}\overset{i.i.d}{\sim}\mathbb{P}}{\mathbb{E}}[E_{in}(\mathbb{W}_{lin}\ w.r.t\ \mathbb{D})]=noise\ level\cdot(1-\frac{d+1}{N})$<sup><a href="#fn_註4" id="reffn_註4">註4</a></sup><br>$\overline{E_{out}}=noise\ level\cdot(1+\frac{d+1}{N})$<br>( 此式在僅用於Linear Regression，因此證明僅放在註釋中 )</p>
<p>這兩式指出，只要是從 $\mathbb{P}$ 這個分佈出來的，且尺寸為 $N$ ，那麼 $E_{in}$ 與 $E_{out}$ 的平均都會跟 $Noise$ 的平均有上述關係。</p>
<p>所以我們可以得出下圖 : </p>
<p><img src="https://i.imgur.com/4nkAl7H.png" alt></p>
<p>當 $N\longrightarrow\infty$ ，$\overline{E_{in}}$ 與 $\overline{E_{out}}\longrightarrow\sigma^2$</p>
<p>$\Longrightarrow\overline{E_{out}}$ 會被限制住</p>
<p>$\Longrightarrow$ Linear Regression 是可行的 !</p>
<h3 id="Linear-Regression-for-Binary-Classification"><a href="#Linear-Regression-for-Binary-Classification" class="headerlink" title="Linear Regression for Binary Classification"></a>Linear Regression for Binary Classification</h3><p><img src="https://i.imgur.com/PuLGZVT.png" alt></p>
<p>$\Longrightarrow err_{0/1}=([![sign(\mathbb{W}^T\mathbb{X})\neq y_n]!])\leq err_{sqr}=(\mathbb{W}^T\mathbb{X}-y_n)^2$</p>
<p>$\Longrightarrow\ classification\ E_{out}\leq\ classification\ E_{in}+\Omega(N,\mathbb{H},\delta)\leq\ regression\ E_{in}+\Omega(N,\mathbb{H},\delta)$</p>
<p>$\Longrightarrow$ Linear regression 的誤差上界的確比 classification 的誤差上界來的大，但是其為解析解，非常容易可以求出解，所以用一些準確度來換得效率也是一個可以接受的選項，誘惑者可先用regression求出解析解後，再拿這個解作為初始值放入PLA去求得更好的解，這樣也可以避免PLA花太多時間再做迭代。</p>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>在現實生活中，我們有時候會想知道的事情不是單純的、絕對的、硬性的是非問題 ( 病人是否活著? )，或許我們會希望能夠有比較 soft 的分類 ( 病人活著的機率? )，或許，我們可以把這樣的 soft binary classification 視為是帶有 $Noise$ 的 classification ( <a href="https://hackmd.io/s/rJaSpwpFV" target="_blank" rel="noopener">機器學習基石筆記-5</a> 內談到的 “翻轉”機率 flipping noise level )。</p>
<p>$\mathbb{X}\overset{i.i.d}{\sim}\mathbb{P}$ , $\mathbb{Y}\overset{i.i.d}{\sim}\mathbb{P}(\mathbb{Y}\mid\mathbb{X})$</p>
<p>$\Longrightarrow\begin{cases}Binary\ classification\ f(\mathbb{X})=Sign(\mathbb{P}(+1\mid\mathbb{X})-\frac{1}{2})\in\left\{+1,-1\right\}\\Soft\ binary\ classification\ f(\mathbb{X})=\mathbb{P}(+1\mid\mathbb{X})\in\left[0,1\right]\end{cases}$</p>
<p><img src="https://i.imgur.com/kr4qkyR.png" alt></p>
<p>上圖白話翻譯就是 : 對於每一個特徵，我們一樣可以給予不同的權重，計算出一個分數，再將這分數經由一個 連續(conti) , 可微(diff) , 單調(monotonic) $\theta$ 函數轉換成機率 ( $\theta:\mathbb{R}\rightarrow\left[0,1\right]$)，這樣的 $\theta$ 函數最常用的是 $Sigmoid\ function=\theta(s)=\frac{1}{1+e^s}$</p>
<p><img src="https://i.imgur.com/N4eRcKO.png" alt></p>
<h3 id="Logistic-Regression-Algorithm"><a href="#Logistic-Regression-Algorithm" class="headerlink" title="Logistic Regression Algorithm"></a>Logistic Regression Algorithm</h3><p>這樣機率型態的 error measure 我們使用 cross-entropy ( 交叉熵 ) error</p>
<p>$E_{in}(\mathbb{W})=cross-entropy=\frac{1}{N}\sum\limits_{n=1}^{N}\ln(1+e^{-y_n\mathbb{W}^T\mathbb{X_n}})$<sup><a href="#fn_註5" id="reffn_註5">註5</a></sup></p>
<p><img src="https://i.imgur.com/LqMNoeq.png" alt></p>
<p>挑選一個初始值 $\mathbb{W}_0$<br>Step 1 : 計算當下 $\mathbb{W}_t$ 的梯度 $\nabla E_{in}(\mathbb{W}_t)=\frac{1}{N}\sum\limits_{n=1}^{N}\theta(-y_n\mathbb{W}^T\mathbb{X})(-y_n\mathbb{X}_n)$</p>
<p>Step 2 : 進行權重迭代更新 $\mathbb{W}_{t+1}=\mathbb{W}_t-\eta\cdot\nabla E_{in}(\mathbb{W}_t)$</p>
<p>停止條件 :<br>(1) $\nabla E_{in}(\mathbb{W}_{t+1})=0$<br>(2) 迭代次數夠多</p>
<hr>
<p>$Proof$</p>
<p>$\because$ 我們要找出 $\min ( cross-entropy\ error)=\min(\frac{1}{N}\sum\limits_{n=1}^{N}\ln(1+e^{(-y_n\mathbb{W}^T\mathbb{X})})$ 以求出 $g$</p>
<p>$\therefore \nabla E_{in}(\mathbb{W})\overset{let}{=}0$</p>
<p>$\frac{\partial E_{in}}{\partial w_i}(\mathbb{W})=\frac{1}{N}\sum\frac{\partial\ln(A)}{\partial A}\cdot\frac{\partial A}{\partial B}\cdot\frac{\partial B}{\partial w_i}$ , where A$=1+e^{(-y_n\mathbb{W}^T\mathbb{X})}$ , B$=-y_n\mathbb{W}^T\mathbb{X}$<br>$=\frac{1}{N}\sum\frac{1}{A}\cdot e^B\cdot(-y_n\cdot x_{n_i})$<br>$=\frac{1}{N}\sum\frac{e^B}{1+e^B}\cdot(-y_n\cdot x_{n_i})$<br>$=\frac{1}{N}\sum\theta(B)\cdot(-y_n\cdot x_{n_i})$<br>$\Longrightarrow\nabla E_{in}(\mathbb{W})=\frac{1}{N}\sum\theta(-y_n\mathbb{W}^T\mathbb{X})\cdot(-y_n\cdot x_{n_i})\overset{let}{=}0$</p>
<p>在這裡有兩種可能性 : </p>
<p>Case 1 :<br>$\theta(-y_n\mathbb{W}^T\mathbb{X})=0\ ,\ \forall\mathbb{W}\Longrightarrow y_n\mathbb{W}^T\mathbb{X}\longrightarrow +\infty\Longrightarrow\mathbb{D}$ is linear separable<br>( $\because\forall i$ , $y_i$ 與 $\mathbb{W}^T\mathbb{X}$ 同號 )</p>
<p>Case 2 :<br>$\frac{1}{N}\sum\theta(-y_n\mathbb{W}^T\mathbb{X})\cdot(-y_n\cdot x_{n_i})=0\Longrightarrow$ 非線性方程求解困難不可行</p>
<p>這邊我們可以發現想要找到類似 Linear Regression 的解析解會非常窒礙難行，所以我們要利用類似PLA的方式做迭代優化 : </p>
<p>$\mathbb{W}_{t+1}=\mathbb{W}_t+\eta’\cdot\nu$</p>
<h3 id="Gradient-Descent-梯度下降註6"><a href="#Gradient-Descent-梯度下降註6" class="headerlink" title="Gradient Descent 梯度下降註6"></a>Gradient Descent 梯度下降<sup><a href="#fn_註6" id="reffn_註6">註6</a></sup></h3><h4 id="How-to-choose-eta-amp-nu"><a href="#How-to-choose-eta-amp-nu" class="headerlink" title="How to choose $\eta$ &amp; $\nu$ ?"></a>How to choose $\eta$ &amp; $\nu$ ?</h4><p>Assume $\left\lVert\nu\right\rVert=1$ and  $\eta’&gt;0$ is small enough</p>
<p>$E_{in}(\mathbb{W}_{t+1})=E_{in}(\mathbb{W}_t+\eta’\cdot\nu)\approx E_{in}(\mathbb{W}_{t+1})+\eta’\cdot\nu^T\cdot\nabla E_{in}(\mathbb{W}_t)$</p>
<p>( $\eta’$ : 大小 , $\nu$ : 方向 , $\nabla E_{in}(\mathbb{W}_t) : 角度$)</p>
<h4 id="方向-nu"><a href="#方向-nu" class="headerlink" title="方向 $\nu$ :"></a>方向 $\nu$ :</h4><p>最好的方向是 $-\nabla E_{in}(\mathbb{W}_t)$ 且 $\left\lVert\nu\right\rVert=1$</p>
<p>$\therefore\nu=-\frac{\nabla E_{in}(\mathbb{W}_t)}{\left\lVert\nabla E_{in}(\mathbb{W}_t)\right\rVert}$</p>
<h4 id="大小-eta"><a href="#大小-eta" class="headerlink" title="大小 $\eta$ :"></a>大小 $\eta$ :</h4><p>若 $\eta’$ 太小，迭代次數會很多且容易落在相對極小值 ( 而非絕對最小值 )<br>若 $\eta’$ 太大，迭代結果震盪大，不夠穩定<br>因此最好的方式是 $\eta’$ 隨著每一次迭代做調整</p>
<p><img src="https://i.imgur.com/2Ce4cew.png" alt></p>
<p>$\Rightarrow \eta’\varpropto\left\lVert\nabla E_{in}(\mathbb{W}_t)\right\rVert\Rightarrow\eta’=\eta\cdot\left\lVert\nabla E_{in}(\mathbb{W}_t)\right\rVert$</p>
<p>$PS: \eta’$ 隨著迭代不斷更動，但更動”固定”比例為 $\eta$</p>
<p>綜合上述 : </p>
<p>$\mathbb{W}_{t+1}\leftarrow\mathbb{W}_t+\eta’\cdot\nu\Longrightarrow\mathbb{W}_{t+1}\leftarrow\mathbb{W}_t+\eta’\cdot-\frac{\nabla E_{in}(\mathbb{W}_t)}{\left\lVert\nabla E_{in}(\mathbb{W}_t)\right\rVert}\Longrightarrow\mathbb{W}_{t+1}\leftarrow\mathbb{W}_t-\eta\cdot\nabla E_{in}(\mathbb{W}_t)$</p>
<h2 id="註釋"><a href="#註釋" class="headerlink" title="註釋"></a>註釋</h2><p><sup><a href="#fn_註1" id="reffn_註1">註1</a></sup>:<br>可參閱 <a href="https://hackmd.io/s/Sk6y3RMYE" target="_blank" rel="noopener">機器學習基石筆記-4</a> </p>
<p><sup><a href="#fn_註2" id="reffn_註2">註2</a></sup>:<br>擬反矩陣pseudo-inverse 又稱廣義反矩陣<br>$\forall A\in\mathbb{C}^{m\times n}\ ,\ \exists !A^\dagger$ s.t. :<br>$1.AA^{\dagger}A=A$<br>$2.A^{\dagger}AA{^\dagger}=A^{\dagger}$<br>$3.(A^\dagger A)^T=A^\dagger A$<br>$4.(AA^\dagger)^T=AA^\dagger$</p>
<p><sup><a href="#fn_註3" id="reffn_註3">註3</a></sup>:<br>廣義反矩陣在數學上的應用之一便是用來求出最小平方法之解<br>$Assume\ \min\parallel AX-Y\parallel_2\ ,\ then\ X=A^\dagger Y+(I-A^\dagger A)w\ ,\ where\ w\ is\ arbitrary\ vector$</p>
<p><sup><a href="#fn_註4" id="reffn_註4">註4</a></sup>:<br>此證明為簡易證明，雖不夠嚴謹，但仍可有不錯的解釋力<br>$Proof$</p>
<ol>
<li><p>$E_{in}(\mathbb{W}_{lin})=\frac{1}{N}\begin{Vmatrix}\mathbb{Y}-\hat{\mathbb{Y}}\end{Vmatrix}^2=\frac{1}{N}\begin{Vmatrix}\mathbb{Y}-\mathbb{X}\mathbb{X}^\dagger\mathbb{Y}\end{Vmatrix}^2=\frac{1}{N}\begin{Vmatrix}(\mathbb{I}-\mathbb{X}\mathbb{X}^\dagger)\mathbb{Y}\end{Vmatrix}^2=\frac{1}{N}\begin{Vmatrix}(\mathbb{I}-\mathbb{H})\mathbb{Y}\end{Vmatrix}^2$</p>
</li>
<li><p><img src="https://i.imgur.com/30cDMOj.png =290x" alt><img src="https://i.imgur.com/J70UyQU.png =280x" alt></p>
</li>
</ol>
<p>$\because\mathbb{H}\mathbb{Y}=\hat{\mathbb{Y}}\Longrightarrow\mathbb{H}$ transform $\mathbb{Y}$ to $\hat{\mathbb{Y}}$<br>$\because\mathbb{Y}-\hat{\mathbb{Y}}=(\mathbb{I}-\mathbb{H})\mathbb{Y}\Longrightarrow(\mathbb{I}-\mathbb{H})$ transform $\mathbb{Y}$ to $\mathbb{Y}-\hat{\mathbb{Y}}$<br>$Suppose\ that\ f(\mathbb{X})\in\hat{\mathbb{Y}}=\mathbb{X}\mathbb{W}_{lin}$<br>$\therefore(\mathbb{I}-\mathbb{H})$ transform $Noise$ to $\mathbb{Y}-\hat{\mathbb{Y}}$ ( $\because\mathbb{Y}=f(\mathbb{X})+Noise$ )</p>
<ol>
<li>$E_{in}(\mathbb{W}_{lin})=\frac{1}{N}\begin{Vmatrix}\mathbb{Y}-\hat{\mathbb{Y}}\end{Vmatrix}^2=\frac{1}{N}\begin{Vmatrix}(\mathbb{I}-\mathbb{H})\cdot Noise\end{Vmatrix}^2=\frac{1}{N}(N-(d+1))\begin{Vmatrix}Noise\end{Vmatrix}^2$<br>( $\because\begin{Vmatrix}\mathbb{I}-\mathbb{H}\end{Vmatrix}^2=tr(\mathbb{I}-\mathbb{H})=tr(\mathbb{I})-tr(\mathbb{H})=N-(d+1)$ )<br>$\Longrightarrow\overline{E_{in}}=\underset{\mathbb{D}\overset{i.i.d}{\sim}\mathbb{P}}{\mathbb{E}}[E_{in}(\mathbb{W}_{lin}\ w.r.t\ \mathbb{D})]=noise\ level\cdot(1-\frac{d+1}{N})$<br>同理可證 : $\overline{E_{out}}=noise\ level\cdot(1+\frac{d+1}{N})$</li>
</ol>
<p><sup><a href="#fn_註5" id="reffn_註5">註5</a></sup>:<br>Cross-Entropy<br>$f(\mathbb{X})=\mathbb{P}(+1\mid\mathbb{X})\iff\mathbb{P}(\mathbb{Y}\mid\mathbb{X})=\begin{cases}f(\mathbb{X})&amp;y=+1\\1-f(\mathbb{X})&amp;y=-1\end{cases}$<br>$\Longrightarrow\prod_{n=1}^{N}\mathbb{P}(\mathbb{X}_n)\mathbb{P}(y_n\mid\mathbb{X}_n)$<br>$\Longrightarrow\prod_{y_n=+1}\mathbb{P}(\mathbb{X}_n)f(\mathbb{X}_n)\prod_{y_m=-1}\mathbb{P}(\mathbb{X}_m)(1-f(\mathbb{X}_m))$ —-(1)<br>我們希望我們的預測 $h$ 可以有 $f$ 的表現<br>$\therefore\prod_{y_n=+1}\mathbb{P}(\mathbb{X}_n)h(\mathbb{X}_n)\prod_{y_m=-1}\mathbb{P}(\mathbb{X}_m)(1-h(\mathbb{X}_m))$ —- (2)</p>
<ul>
<li>若 $h\approx f$，則上述(1)(2)兩式理應接近 且 (1)式應該會非常接近1</li>
<li>我們將(2)式定義成一個函數 稱為 $likelyhood(h)$，則我們想要的 $g=\arg\max\limits_{\forall h}(likelyhood(h))$<br>$\therefore likelyhood(h)$<br>$=\prod_{y_n=+1}\mathbb{P}(\mathbb{X}_n)h(\mathbb{X}_n)\prod_{y_m=-1}\mathbb{P}(\mathbb{X}_m)(1-h(\mathbb{X}_m))$<br>$=\prod_{y_n=+1}\mathbb{P}(\mathbb{X}_n)h(\mathbb{X}_n)\prod_{y_m=-1}\mathbb{P}(\mathbb{X}_m)$<font color="#dd0000">$(-h(\mathbb{X}_m))$</font><br>$( \because h(\mathbb{X})=\theta(\mathbb{W}^T\mathbb{X})\ and\ 1-\theta(s)=-\theta(s) )$<br>$\propto\prod_{n=1}^{N}h(y_n\mathbb{X}_n)=\prod_{n=1}^{N}\theta(y_n\mathbb{W}^T\mathbb{X})$</li>
</ul>
<p>$g=\arg\max\limits_{\forall h}( likelyhood(h) )$<br>$\Leftrightarrow g=\arg\max\limits_{\forall\mathbb{W}}(\prod_{n=1}^{N}\theta(y_n\mathbb{W}^T\mathbb{X}))$<br>$\Leftrightarrow g=\arg\max\limits_{\forall\mathbb{W}}(\ln\prod_{n=1}^{N}\theta(y_n\mathbb{W}^T\mathbb{X}))$<br>$\Leftrightarrow g=\arg\min\limits_{\forall\mathbb{W}}(\frac{1}{N}\sum\limits_{n=1}^{N}-\ln\theta(y_n\mathbb{W}^T\mathbb{X}))$<br>$=\arg\min\limits_{\forall\mathbb{W}}(\frac{1}{N}\sum\limits_{n=1}^{N}-\ln(1+e^{(-y_n\mathbb{W}^T\mathbb{X})})^{-1})$<br>$\begin{matrix}=\arg\min\limits_{\forall\mathbb{W}}(\underbrace{\frac{1}{N}\sum\limits_{n=1}^{N}\ln(1+e^{(-y_n\mathbb{W}^T\mathbb{X})})})\\cross-entropy\ error\end{matrix}$</p>
<p><sup><a href="#fn_註6" id="reffn_註6">註6</a></sup>:<br>有關於梯度下降法的更詳細討論可以參閱 <a href="https://hackmd.io/s/ryQypiDK4" target="_blank" rel="noopener">Gradient descent 梯度下降</a> 一文</p>

    </div>

    
    
    
        
      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>作者： </strong>Allen Tzeng</li>
  <li class="post-copyright-link">
    <strong>文章連結：</strong>
    <a href="https://allen108108.github.io/blog/2019/10/07/林軒田機器學習基石筆記 - 第九講、第十講/" title="林軒田機器學習基石筆記 - 第九講、第十講">https://allen108108.github.io/blog/2019/10/07/林軒田機器學習基石筆記 - 第九講、第十講/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版權聲明： </strong>本網誌所有文章除特別聲明外，均採用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh_TW" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 許可協議。轉載請註明出處！</li>
</ul>
</div>

      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/blog/2019/10/07/利用 ImageDataGenerator (資料增強) 加強 CNN 辨識率/" rel="next" title="利用 ImageDataGenerator (資料增強) 加強 CNN 辨識率">
                  <i class="fa fa-chevron-left"></i> 利用 ImageDataGenerator (資料增強) 加強 CNN 辨識率
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/blog/2019/10/07/林軒田機器學習基石筆記 - 第十一講/" rel="prev" title="林軒田機器學習基石筆記 - 第十一講">
                  林軒田機器學習基石筆記 - 第十一講 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Regression"><span class="nav-number">1.</span> <span class="nav-text">Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Regression-Algorithm"><span class="nav-number">1.1.</span> <span class="nav-text">Linear Regression Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Regression-的可行性"><span class="nav-number">1.2.</span> <span class="nav-text">Linear Regression 的可行性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Regression-for-Binary-Classification"><span class="nav-number">1.3.</span> <span class="nav-text">Linear Regression for Binary Classification</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Regression"><span class="nav-number">2.</span> <span class="nav-text">Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-Regression-Algorithm"><span class="nav-number">2.1.</span> <span class="nav-text">Logistic Regression Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Descent-梯度下降註6"><span class="nav-number">2.2.</span> <span class="nav-text">Gradient Descent 梯度下降註6</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#How-to-choose-eta-amp-nu"><span class="nav-number">2.2.1.</span> <span class="nav-text">How to choose $\eta$ &amp; $\nu$ ?</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#方向-nu"><span class="nav-number">2.2.2.</span> <span class="nav-text">方向 $\nu$ :</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#大小-eta"><span class="nav-number">2.2.3.</span> <span class="nav-text">大小 $\eta$ :</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#註釋"><span class="nav-number">3.</span> <span class="nav-text">註釋</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/blog/images/allen.jpg"
      alt="Allen Tzeng">
  <p class="site-author-name" itemprop="name">Allen Tzeng</p>
  <div class="site-description" itemprop="description">Study about Mathematics , Programming and Data Science</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/blog/categories/">
          
        
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分類</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://allen108108.github.io/" title="Github page &rarr; https://allen108108.github.io/"><i class="fa fa-fw fa-github-alt"></i>Github page</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/allen108108" title="GitHub &rarr; https://github.com/allen108108" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:allen108108@hotmail.com" title="E-Mail &rarr; mailto:allen108108@hotmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.facebook.com/Mathpy-257929091666547/" title="FB Page &rarr; https://www.facebook.com/Mathpy-257929091666547/" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i>FB Page</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Allen Tzeng</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 強力驅動 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主題 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.4.0</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="訪客總數">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="總瀏覽次數">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='255, 80, 219' opacity='1' zIndex='-1' count='100' src="/blog/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>
  <script src="/blog/lib/anime.min.js?v=3.1.0"></script>
  <script src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/blog/js/utils.js?v=7.4.0"></script><script src="/blog/js/motion.js?v=7.4.0"></script>
<script src="/blog/js/schemes/pisces.js?v=7.4.0"></script>

<script src="/blog/js/next-boot.js?v=7.4.0"></script>



  





















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

</body>
</html>
