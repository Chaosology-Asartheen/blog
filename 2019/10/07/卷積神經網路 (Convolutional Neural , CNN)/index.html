<!DOCTYPE html>





<html lang="zh-TW">
<head>
  <meta name="google-site-verification" content="exmdvS3MT1hVojBxSEZZZ8TIBBOg6pJ2POTtzNuecqs" />
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/blog/images/logo.svg?v=7.4.0" color="#222">

<link rel="stylesheet" href="/blog/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/blog/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/blog/',
    scheme: 'Gemini',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","width":280,"display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '複製',
      copy_success: '複製成功',
      copy_failure: '複製失敗'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="本文內容以李宏毅 Machine Learning (2017) 第十講 : Convolutional Neural Network ( CNN ) 為主，搭配參考資料編輯而成。  本篇圖片部分出自 Machine Learning (2017) 課程內容講義">
<meta property="og:type" content="article">
<meta property="og:title" content="卷積神經網路 (Convolutional Neural , CNN)">
<meta property="og:url" content="https://allen108108.github.io/blog/2019/10/07/卷積神經網路 (Convolutional Neural , CNN)/index.html">
<meta property="og:site_name" content="Math.py">
<meta property="og:description" content="本文內容以李宏毅 Machine Learning (2017) 第十講 : Convolutional Neural Network ( CNN ) 為主，搭配參考資料編輯而成。  本篇圖片部分出自 Machine Learning (2017) 課程內容講義">
<meta property="og:locale" content="zh-TW">
<meta property="og:image" content="https://i.imgur.com/zVa11Vr.png">
<meta property="og:image" content="https://i.imgur.com/77bPBr3.png">
<meta property="og:image" content="https://i.imgur.com/JTqr2Yw.png">
<meta property="og:image" content="https://i.imgur.com/v4VM3qu.gif">
<meta property="og:image" content="https://i.imgur.com/z4wLQJn.png">
<meta property="og:image" content="https://i.imgur.com/0xP0Ins.png =350x">
<meta property="og:image" content="https://i.imgur.com/3YHkC4o.gif =400x">
<meta property="og:image" content="https://i.imgur.com/m1qlenn.jpg">
<meta property="og:image" content="https://i.imgur.com/UST7EeO.png =350x">
<meta property="og:image" content="https://i.imgur.com/xRiuMSa.png">
<meta property="og:image" content="https://i.imgur.com/8nFL49Q.png =350x">
<meta property="og:image" content="https://i.imgur.com/EyuzvUr.png =350x">
<meta property="og:image" content="https://i.imgur.com/YdM0KdM.png">
<meta property="og:image" content="https://i.imgur.com/laTqCxB.jpg">
<meta property="og:image" content="https://i.imgur.com/7iq1A6K.jpg">
<meta property="og:image" content="https://i.imgur.com/88V2T2X.png =400x">
<meta property="og:image" content="https://i.imgur.com/gs0K20B.png">
<meta property="og:image" content="https://i.imgur.com/Ok5FuhJ.png =450x">
<meta property="og:image" content="https://i.imgur.com/crpxYlI.jpg">
<meta property="og:image" content="https://i.imgur.com/QCJRHPL.png">
<meta property="og:image" content="https://i.imgur.com/VeWOyba.png =350x">
<meta property="og:image" content="https://i.imgur.com/EwFeKmh.png">
<meta property="og:image" content="https://i.imgur.com/TDpCK3e.png">
<meta property="og:image" content="https://i.imgur.com/IjfZQbT.png">
<meta property="og:image" content="https://i.imgur.com/xhPyOiU.png">
<meta property="og:image" content="https://i.imgur.com/gwyPhGV.png">
<meta property="og:updated_time" content="2019-10-22T08:00:00.046Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="卷積神經網路 (Convolutional Neural , CNN)">
<meta name="twitter:description" content="本文內容以李宏毅 Machine Learning (2017) 第十講 : Convolutional Neural Network ( CNN ) 為主，搭配參考資料編輯而成。  本篇圖片部分出自 Machine Learning (2017) 課程內容講義">
<meta name="twitter:image" content="https://i.imgur.com/zVa11Vr.png">
  <link rel="canonical" href="https://allen108108.github.io/blog/2019/10/07/卷積神經網路 (Convolutional Neural , CNN)/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>卷積神經網路 (Convolutional Neural , CNN) | Math.py</title>
  
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-149442581-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-149442581-1');
    }
  </script>








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-TW">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/blog/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Math.py</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Wir müssen wissen , wir werden wissen</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切換導航欄">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/blog/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首頁</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/blog/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分類</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/blog/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>歸檔</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://allen108108.github.io/blog/blog/2019/10/07/卷積神經網路 (Convolutional Neural , CNN)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Allen Tzeng">
      <meta itemprop="description" content="Study about Mathematics , Programming and Data Science">
      <meta itemprop="image" content="/blog/images/allen.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Math.py">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">卷積神經網路 (Convolutional Neural , CNN)

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">發表於</span>

              
                
              

              <time title="創建時間：2019-10-07 03:08:31" itemprop="dateCreated datePublished" datetime="2019-10-07T03:08:31+08:00">2019-10-07</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新於</span>
                <time title="修改時間：2019-10-22 16:00:00" itemprop="dateModified" datetime="2019-10-22T16:00:00+08:00">2019-10-22</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分類於</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/課程筆記-Course/" itemprop="url" rel="index"><span itemprop="name">課程筆記 Course</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/blog/categories/課程筆記-Course/李宏毅-Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">李宏毅 Machine Learning</span></a></span>

                
                
              
            </span>
          

          
            <span class="post-meta-item" title="閱讀次數" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">閱讀次數：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/blog/2019/10/07/卷積神經網路 (Convolutional Neural , CNN)/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/10/07/卷積神經網路 (Convolutional Neural , CNN)/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<ul>
<li><p><strong>本文內容以李宏毅 Machine Learning (2017) 第十講 : Convolutional Neural Network ( CNN ) 為主，搭配參考資料編輯而成。</strong></p>
</li>
<li><p><strong>本篇圖片部分出自 Machine Learning (2017) 課程內容講義</strong></p>
</li>
</ul>
</blockquote>
<a id="more"></a>
<p>當我們剛開始接觸深度學習的時候，最常看到的例子便是使用 MINST 資料庫進行手寫數字的辨識。概念如下圖所示，將所有像素灰階數值壓成一維資料後再丟進全連接層進行學習。</p>
<p><img width="500" src="https://i.imgur.com/zVa11Vr.png"><br>(圖片來源 : 3Blue1Brown Youtube : <a href="https://youtu.be/aircAruvnKk" target="_blank" rel="noopener">究竟神經網路是什麼？  第一章 深度學習</a>)</p>
<p>如果在進行一般的圖片辨識時，我們不會使用上面的方式，因為這樣子做會有幾個問題 : </p>
<ol>
<li><p>在一般的圖片辨識問題中，事實上會有一些 pattern 可能會出現在圖片中的某個部位，且這樣的 pattern 可能由許多個鄰近像素構成，如果依照上面的方式會破壞這樣的 pattern 結構。</p>
</li>
<li><p>全連接層搭配高像素的圖片，會讓整個計算成本大幅增加。</p>
</li>
</ol>
<p>基於上面幾個理由便衍伸出 Convolutional Neural Network ( CNN ) 卷積神經網路來進行圖像辨識。</p>
<p>整個 CNN 結構主要分成幾個部分 :<br><strong>卷積層 ( Convolution layer )、池化層 (Pooling layer) 以及最後一個全連接層 ( Fully Connected layer )</strong>。</p>
<p><img width="500" src="https://i.imgur.com/77bPBr3.png"></p>
<h2 id="Convolution-Layer-卷積層"><a href="#Convolution-Layer-卷積層" class="headerlink" title="Convolution Layer 卷積層"></a>Convolution Layer 卷積層</h2><p>卷積層主要是由許多不同的 kernel 在輸入圖片上進行卷積運算。</p>
<p>什麼是卷積 ? 在這邊卷積其實就是兩個步驟組成的運算 : 滑動 + 內積，利用 filter 在輸入圖片上滑動並且持續進行矩陣內積，卷積後得到的圖片我們稱之為 feature map。<sup><a href="#fn_註1" id="reffn_註1">註1</a></sup></p>
<p><img src="https://i.imgur.com/JTqr2Yw.png" alt><br><img src="https://i.imgur.com/v4VM3qu.gif" alt><br>(圖片取自 : <a href="https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-convolution-neural-network-%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-bfa8566744e9" target="_blank" rel="noopener">[ 機器學習 ML NOTE ] Convolution Neural Network 卷積神經網路</a>)</p>
<p>kernel 我們也稱為 filter ， 我認為 filter 這樣的概念可以讓人更體會卷積層的作用，一般我們使用修圖軟體的各種濾鏡功能即是不同的 kernel 在圖片上作用後的結果。<br>( 想知道許多不同的 filter 造成的效果可以參考 Wikipedia — <a href="https://en.wikipedia.org/wiki/Kernel_%28image_processing%29" target="_blank" rel="noopener">Kernel ( image processing ) </a>  )</p>
<p>在全連接層神經網路中經由學習不斷更新的權重，在 CNN 這邊指的就是 filter ，如上圖 3X3 的 filter 內就相當於有 9 個權重。我們可以想像，CNN 訓練的過程就是不斷地在改變 filter 來凸顯這個輸入圖像上的特徵。</p>
<p>不過，CNN 還是有一些值得注意的地方 : </p>
<p><strong>1. 每一層卷積層的 filter 不會只有一個</strong></p>
<p>我們引用 CNN 的經典論文 GradientBased Learning Applied to Document Recognition 中的 LeNet-5 結構，第一層的卷積層就給了 6 個 3X3 kernel，也就是說在這同一層中就有 54 個權重需要同時更新，而這六個 filter 也會相對應給出六個 feature map 。</p>
<p><img src="https://i.imgur.com/z4wLQJn.png" alt><br>(圖片來源 : Yann LeCun Leon Bottou Yoshua Bengio and Patrick Haffner.(1998).<em>GradientBased Learning Applied to Document Recognition</em><sup><a href="#fn_註2" id="reffn_註2">註2</a></sup>)</p>
<p><strong>2. 彩色的圖片同時會有 RGB 三個 channel</strong></p>
<p>也就是說，我們每一張圖片都會有深度，既然輸入圖有深度，那麼我們的 filter 也相同的會有一個深度，每一個 channel 會對應著 filter 的深度做卷積，filter 每一個深度的數值可能會不一樣。</p>
<p><img src="https://i.imgur.com/0xP0Ins.png =350x" alt></p>
<p>攤開看大概會像這樣子</p>
<p><img src="https://i.imgur.com/3YHkC4o.gif =400x" alt><br>(圖片取自 : <a href="http://machinelearninguru.com/computer_vision/basics/convolution/convolution_layer.html" target="_blank" rel="noopener">Undrestanding Convolutional Layers in Convolutional Neural Networks (CNNs)</a>)</p>
<p><strong>3. 經過卷積層過後的 feature map 會變得比原尺寸小</strong></p>
<p>這其實不難看出來，只要親自做一輪卷積運算便可以很容易地發現這個結論，如果不想要讓整個 feature map 縮小，我們可以使用 padding 的方法 ( 於輸入圖片的周圍補上一圈 0 )，來避免 feature map 縮小的狀況。</p>
<p><img src="https://i.imgur.com/m1qlenn.jpg" alt></p>
<p><strong>4. filter 滑動的步伐 (Stride) 不一定必須是 1</strong></p>
<p>當我們滑動的步伐加大，那麼出來的 feature map 尺寸會相對縮得更小</p>
<p><img src="https://i.imgur.com/UST7EeO.png =350x" alt></p>
<h3 id="卷積層的特色"><a href="#卷積層的特色" class="headerlink" title="卷積層的特色"></a>卷積層的特色</h3><p><strong>1. 可以保留圖片中的空間結構，並從這樣的結構中萃取出特徵</strong></p>
<p><img src="https://i.imgur.com/xRiuMSa.png" alt></p>
<p>每一個 filter 我們可以視為是一個圖像上的 「pattern」，經過卷積運算的滑動過程，其實就是在檢驗每一個圖片的部分是否有符合這個 pattern 的區域，有相同 pattern 出現的區域便在 feature map 上有相對高的分數，最後我們也是經由整體分數的高低來檢驗並調整 filter。</p>
<p><img src="https://i.imgur.com/8nFL49Q.png =350x" alt><br>( feature map 中的數值總和我們可以當作這個 filter 的 degree，我們希望找到的 filter 能讓 feature map 的數值和最大 )</p>
<p><strong>2. 利用權重共享的方式減少參數</strong></p>
<p>從李宏毅 Machine Learning 課程中的講義可以看出來「權重共享」的概念是什麼意思。當我們進行 filter 滑動的時候，也是表示我們同一組權重正作用在不同區域，這樣的過程我們這個 filter 並沒有改變，也就是權重是不會變動的。</p>
<p>下圖中，李宏毅將整個卷積層排列成類似全連接層的型態，這樣可以很明白的看出共享權重的過程。</p>
<p><img src="https://i.imgur.com/EyuzvUr.png =350x" alt></p>
<p><em>在這裡我想補充一點，我自己在自學卷積這一部分時，一直對於卷積過後的數值與呈現出來的 feature map 之間的關聯性沒有很明確的理解，以上圖來看，經過卷積運算後的 3X3 矩陣表示的到底是什麼東西 ?</em></p>
<p><em>以下幾個卷積層操作影片可以幫助我們對於整個卷積層的運作有更深刻的理解</em><sup><a href="#fn_註3" id="reffn_註3">註3</a></sup></p>
<div class="video-container"><iframe src="//www.youtube.com/embed/KiftWz544_8" frameborder="0" allowfullscreen></iframe></div>
<p>(影片來源 : Henry Warren — <a href="https://www.youtube.com/watch?v=KiftWz544_8" target="_blank" rel="noopener">The Convolution Layer (CNN Visualization)</a>)</p>
<div class="video-container"><iframe src="//www.youtube.com/embed/z6k_RMKExlQ" frameborder="0" allowfullscreen></iframe></div>
<p>(影片來源 : Gene Kogan — <a href="https://www.youtube.com/watch?v=z6k_RMKExlQ&amp;" target="_blank" rel="noopener">ml4a @ itp nyu :: 03 convolutional neural networks</a> )</p>
<hr>
<h3 id="補充"><a href="#補充" class="headerlink" title="[ 補充 ]"></a>[ 補充 ]</h3><p>最近在看 “ <em>Network In Network</em> “ 以及 “ <em>Going deeper with convolutions.</em> “ 這兩篇論文的時候，又重新審視了一下自己對於 CNN 的了解，這才發現，如果沒有辦法理解 $1\times 1$ Convolution Layer 的作用的話，對於 CNN 的了解確實是不足的。因此特別在這裡做了一個補充。</p>
<p><strong>「你認為，經過一個卷積層後的輸出，究竟有幾張圖像 ?」</strong></p>
<p><img src="https://i.imgur.com/YdM0KdM.png" alt></p>
<p>我們拿 “ <em>Gradient-Based Learning Applied to Document Recognition</em> “ 論文內的 LeNet-5 model 來看看。就單看第一層卷積層吧，一張 $32\times 32$ 的輸入，經過了五個 $28\times 28$ 的 Kernel，形成了五張 feature maps，所以第一層卷積層應該是輸出五張圖像，看上圖也似乎是如此沒有錯。</p>
<p>不過真的是這樣嗎 ? </p>
<p>事實上，這五張 feature maps 的確是卷積後的結果，但並非整個卷積層最後的輸出，卷積層還必須將這五張 feature maps 疊合在一起，這才是真正第一層卷積層的輸出。</p>
<p><strong>也就是說，輸出仍然為一張圖像，但是變厚了</strong></p>
<p>我自己會把它想像成，一張完整的圖像，會是許多特徵疊加後的結果，而卷積層也會做到這個工作。</p>
<p>所以，當我們卷積層的 Kernel 數量越多，輸出的圖像就會越厚 ( 專業的講法是這圖像的深度變得更深了 )，相對的參數也會越多。</p>
<p><img src="https://i.imgur.com/laTqCxB.jpg" alt><br>( 圖片來源 : <a href="https://medium.com/@chih.sheng.huang821/%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-convolutional-neural-network-cnn-1-1%E5%8D%B7%E7%A9%8D%E8%A8%88%E7%AE%97%E5%9C%A8%E5%81%9A%E4%BB%80%E9%BA%BC-7d7ebfe34b8" target="_blank" rel="noopener">卷積神經網路(Convolutional neural network, CNN): 1×1卷積計算在做什麼</a> )</p>
<h3 id="1-times-1-卷積層到底在做什麼"><a href="#1-times-1-卷積層到底在做什麼" class="headerlink" title="$1\times 1$ 卷積層到底在做什麼 ?"></a>$1\times 1$ 卷積層到底在做什麼 ?</h3><p>如果我們把卷積層只看作是一個逐步向量內積的過程，那一定會覺得 $1\times 1$ 卷積層只是單純的純量相乘的結果而已。當然，就運算上的確只是如此，但是如果我們了解了真正卷積層的運作，你就不難發現，$1\times 1$ 卷積層還能有一個非常好用的功能。</p>
<p><strong>降維 Dimension Reduction</strong></p>
<p><img src="https://i.imgur.com/7iq1A6K.jpg" alt><br>( 圖片來源 : <a href="https://medium.com/@chih.sheng.huang821/%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-convolutional-neural-network-cnn-1-1%E5%8D%B7%E7%A9%8D%E8%A8%88%E7%AE%97%E5%9C%A8%E5%81%9A%E4%BB%80%E9%BA%BC-7d7ebfe34b8" target="_blank" rel="noopener">卷積神經網路(Convolutional neural network, CNN): 1×1卷積計算在做什麼</a> )</p>
<p>上圖作者認為這樣的表示方式比較容易理解，但我個人還是比較建議以下圖的方式來呈現，原因無它，下圖比較能真正表現出圖像「深度」的概念。</p>
<p><img src="https://i.imgur.com/88V2T2X.png =400x" alt></p>
<p>經過了一個 $1\times 1$ 卷積層後，圖像大小不變，但卻能有效的控制參數的增加，這樣的方式在 “ <em>Network In Network</em> “ 以及 “ <em>Going deeper with convolutions.</em> “ 這兩篇論文中有效的控制了整個模型的複雜度。</p>
<hr>
<h2 id="Pooling-Layer-池化層"><a href="#Pooling-Layer-池化層" class="headerlink" title="Pooling Layer 池化層"></a>Pooling Layer 池化層</h2><p><img src="https://i.imgur.com/gs0K20B.png" alt></p>
<p>池化層的主要概念是，當我們在做圖片的特徵萃取的過程中，圖形的縮放應該不會影響到我們的目的，經由這樣的 scaling 我們也可以再一次的減少神經網路的參數。</p>
<p>常用的 pooling 方式有 Max pooling 與 Average pooling :<br>如同卷積層，pooling 也有一個 kernel，也是在輸入圖像上進行滑動運算但和卷積層不同的地方是滑動方式不會互相覆蓋，且運算是以 kernel 涵蓋範圍的最大值 ( Max pooling ) 或平均值 ( Average pooling )。</p>
<p><img src="https://i.imgur.com/Ok5FuhJ.png =450x" alt><br>(圖片來源 : Quora — <a href="https://www.quora.com/What-is-the-benefit-of-using-average-pooling-rather-than-max-pooling" target="_blank" rel="noopener">What is the benefit of using average pooling rather than max pooling?</a>)</p>
<p>許多研究都發現，Max pooling 的效果會比 Average pooling 來的好，因此現在大多使用 Max pooling 。( LeNet-5 則是使用 Average pooling )</p>
<h3 id="池化層的特色"><a href="#池化層的特色" class="headerlink" title="池化層的特色"></a>池化層的特色</h3><p><strong>1. 藉由對輸入圖片的 subsampling 來減少參數，減少計算成本</strong></p>
<p>從 pooling 過程來看，如果原本的輸入圖片，經過 2x2 pooling kernel 之後，整個圖片的尺寸將會縮小 $\displaystyle{\frac{1}{2}}$，如此一來，計算成本也將節省一半。</p>
<p><strong>2. 具有特徵不變性</strong></p>
<p>以下面三個圖來看看，經過池化後如何保持特徵不變性，圖一平移，圖二旋轉以及圖三是縮小。</p>
<p>我們可以發現，經過池化後，這些特徵最後呈現的 feature map 會是相同的，這也符合了我們當初對於池化層的期待與概念，經過 subsampling 後相對於原圖並不會有什麼太多的改變。</p>
<p><img src="https://i.imgur.com/crpxYlI.jpg" alt><br>(圖片來源 : 知乎 — <a href="https://www.zhihu.com/question/36686900" target="_blank" rel="noopener">CNN网络的pooling层有什么用？</a>)</p>
<p><strong>3. 提高 Receptive Field (中國翻譯為 : 感受野)</strong></p>
<p>Receptive field 指的是我們在 feature map 中的一個像素內能看到輸入圖像區域</p>
<p><img src="https://i.imgur.com/QCJRHPL.png" alt></p>
<p>不管卷積層或是池化層都能夠有提高 Receptive field 的作用，所謂的「提高 Receptive field」，依上圖來看就是當我們經過池化或是卷積層後，feature map 對應的原圖區域就會越大，也就是我們可以「見微知著」，從一個點看到整個原圖蘊含的資訊。</p>
<p>這個有趣的影片我認為也可以展示 pooling layer subsampling 的概念</p>
<div class="video-container"><iframe src="//www.youtube.com/embed/fApFKmXcp2Y" frameborder="0" allowfullscreen></iframe></div>
<h2 id="Fully-Connected-Layer-全連接層"><a href="#Fully-Connected-Layer-全連接層" class="headerlink" title="Fully Connected Layer 全連接層"></a>Fully Connected Layer 全連接層</h2><p>這邊的全連接層跟我們進行手寫辨識的方式一樣，說穿了就是一個分類器，把我們經過數個卷積、池化後的結果進行分類。</p>
<p>這邊可能會有人有一個問題 : 「 不是說全連接層會破壞圖片的空間結構嗎 ? 」</p>
<p>是的，但是別忘了，我們已經經過數個卷積池化層了，現在每一個節點已經包含了空間結構的資訊在裡面，這個時候進行全連接層的分類就不會有破壞空間結構的問題。</p>
<p>在全連接層有趣的地方是，如果我們把這一層的輸出視覺化，直覺上會認為他輸出的視覺化圖形會接近我們認知的分類。舉例來說，我們丟手寫數字進去經過 CNN 最後全連接層的輸出，理應出來的圖形會接近我們認知的數字型態。然而我們實際將全連接層最後輸出視覺化會是下圖。</p>
<p><img src="https://i.imgur.com/VeWOyba.png =350x" alt></p>
<p>問題在於，CNN 與人腦的視覺模式有很大的不同，這樣的結論在許多的論文中都有被提及。<sup><a href="#fn_註4" id="reffn_註4">註4</a></sup></p>
<p>我們如果將上圖加上ㄧ些 constrain (Regularization)，來告知系統說有些點不是我們需要的部分，就可以稍微看出一些端倪。</p>
<p><img src="https://i.imgur.com/EwFeKmh.png" alt></p>
<h2 id="CNN-的應用"><a href="#CNN-的應用" class="headerlink" title="CNN 的應用"></a>CNN 的應用</h2><p>我們了解到 CNN 對於空間結構上有非常好的表現，因此，需要保留空間結構的學習模式， CNN 就有機會在這樣的條件下訓練出非常好的 model。</p>
<h3 id="應用一-Alpha-Go"><a href="#應用一-Alpha-Go" class="headerlink" title="應用一 : Alpha Go"></a>應用一 : Alpha Go</h3><p><img src="https://i.imgur.com/TDpCK3e.png" alt></p>
<p>如圖所示，棋盤本身就具有一定的空間結構，而 CNN 也的確在下棋這件事上面有著很不錯的表現，我們可以餵進許多的棋譜讓它學習到各種狀況下的下棋策略。</p>
<p>然而，我們在 CNN 上面有經過 Max pooling subsampling 來縮小尺度保留特徵，棋盤似乎無法這樣做。</p>
<p>我們從 Aloha Go 的結構也發現，他們的確將 pooling layer 拿掉了。</p>
<p><img src="https://i.imgur.com/IjfZQbT.png" alt></p>
<h3 id="應用二-語音辨識"><a href="#應用二-語音辨識" class="headerlink" title="應用二 : 語音辨識"></a>應用二 : 語音辨識</h3><p><img src="https://i.imgur.com/xhPyOiU.png" alt></p>
<p>由於人類的說話本身也具有結構，因此利用語音建立的 Spectrogram 來進行 CNN 的訓練。</p>
<h3 id="應用三-文本分析"><a href="#應用三-文本分析" class="headerlink" title="應用三 : 文本分析"></a>應用三 : 文本分析</h3><p><img src="https://i.imgur.com/gwyPhGV.png" alt></p>
<p>跟上面語音的概念相似，文本也具有類似的空間概念，因此也可以利用 CNN 來訓練出一個不錯的文本分析模型。</p>
<h2 id="註釋"><a href="#註釋" class="headerlink" title="註釋"></a>註釋</h2><p><sup><a href="#fn_註1" id="reffn_註1">註1</a></sup>:<br>在數學上對於卷積的定義更加抽象且準確，而卷積這樣的概念也被廣泛的應用在許多領域中，而在 “ 知乎 — <a href="https://www.zhihu.com/question/22298352" target="_blank" rel="noopener">如何通俗易懂地解释卷积？</a> “ 這篇文章中對於卷積有多面向的討論，非常建議大家可以閱讀。</p>
<p><sup><a href="#fn_註2" id="reffn_註2">註2</a></sup>:<br>論文連結 : Yann LeCun Leon Bottou Yoshua Bengio and Patrick Haffner.(1998).<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener"><em>GradientBased Learning Applied to Document Recognition</em></a></p>
<p><sup><a href="#fn_註3" id="reffn_註3">註3</a></sup>:<br>除此之外，對於卷積層的運作也可參考以下文章 :<br>     (1) CH.Tseng — <a href="https://chtseng.wordpress.com/2019/06/02/opencv的捲積操作/?fbclid=IwAR0S1Kdf3pAbrwmiRrXNNSqxIf_GUnat0IcMFkCMxjAy6Z8HYi8kFLXwK0I" target="_blank" rel="noopener">OpenCV的捲積操作</a><br>     (2) Steven Shen — <a href="https://medium.com/@syshen/%E5%85%A5%E9%96%80%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-2-d694cad7d1e5" target="_blank" rel="noopener">入門深度學習 — 2</a></p>
<p><sup><a href="#fn_註4" id="reffn_註4">註4</a></sup>:<br>參考論文 :<br>     (1) Anh Nguyen,Jason Yosinski and Jeff Clune .(2014).<a href="https://arxiv.org/pdf/1412.1897.pdf" target="_blank" rel="noopener"><em>Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images.</em></a><br>     (2)Karen Simonyan, Andrea Vedaldi and Andrew Zisserman .(2013).<em><a href="https://arxiv.org/pdf/1312.6034.pdf" target="_blank" rel="noopener">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.</a></em></p>

    </div>

    
    
    
        
      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>作者： </strong>Allen Tzeng</li>
  <li class="post-copyright-link">
    <strong>文章連結：</strong>
    <a href="https://allen108108.github.io/blog/2019/10/07/卷積神經網路 (Convolutional Neural , CNN)/" title="卷積神經網路 (Convolutional Neural , CNN)">https://allen108108.github.io/blog/2019/10/07/卷積神經網路 (Convolutional Neural , CNN)/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版權聲明： </strong>本網誌所有文章除特別聲明外，均採用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh_TW" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 許可協議。轉載請註明出處！</li>
</ul>
</div>

      

      <footer class="post-footer">

        
  <div class="post-widgets">
    <div class="wp_rating">
      <div id="wpac-rating"></div>
    </div>
  
  </div>

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/blog/2019/10/07/Regularization 方法 _ Weight Decay , Early Stopping and Dropout/" rel="next" title="Regularization 方法 : Weight Decay , Early Stopping and Dropout">
                  <i class="fa fa-chevron-left"></i> Regularization 方法 : Weight Decay , Early Stopping and Dropout
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/blog/2019/10/07/使用 Github page 製作個人網站 – 以 Hexo _ Next 製作部落格/" rel="prev" title="使用 Github page 製作個人網站 -- 以 Hexo / Next 製作部落格">
                  使用 Github page 製作個人網站 -- 以 Hexo / Next 製作部落格 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    
  <div class="comments" id="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Convolution-Layer-卷積層"><span class="nav-number">1.</span> <span class="nav-text">Convolution Layer 卷積層</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#卷積層的特色"><span class="nav-number">1.1.</span> <span class="nav-text">卷積層的特色</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#補充"><span class="nav-number">1.2.</span> <span class="nav-text">[ 補充 ]</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-times-1-卷積層到底在做什麼"><span class="nav-number">1.3.</span> <span class="nav-text">$1\times 1$ 卷積層到底在做什麼 ?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pooling-Layer-池化層"><span class="nav-number">2.</span> <span class="nav-text">Pooling Layer 池化層</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#池化層的特色"><span class="nav-number">2.1.</span> <span class="nav-text">池化層的特色</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fully-Connected-Layer-全連接層"><span class="nav-number">3.</span> <span class="nav-text">Fully Connected Layer 全連接層</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN-的應用"><span class="nav-number">4.</span> <span class="nav-text">CNN 的應用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#應用一-Alpha-Go"><span class="nav-number">4.1.</span> <span class="nav-text">應用一 : Alpha Go</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#應用二-語音辨識"><span class="nav-number">4.2.</span> <span class="nav-text">應用二 : 語音辨識</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#應用三-文本分析"><span class="nav-number">4.3.</span> <span class="nav-text">應用三 : 文本分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#註釋"><span class="nav-number">5.</span> <span class="nav-text">註釋</span></a></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/blog/images/allen.jpg"
      alt="Allen Tzeng">
  <p class="site-author-name" itemprop="name">Allen Tzeng</p>
  <div class="site-description" itemprop="description">Study about Mathematics , Programming and Data Science</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">59</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/blog/categories/">
          
        
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分類</span>
        </a>
      </div>
    
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://allen108108.github.io/" title="Github page &rarr; https://allen108108.github.io/"><i class="fa fa-fw fa-github-alt"></i>Github page</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/allen108108" title="GitHub &rarr; https://github.com/allen108108" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:allen108108@hotmail.com" title="E-Mail &rarr; mailto:allen108108@hotmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.facebook.com/Mathpy-257929091666547/" title="FB Page &rarr; https://www.facebook.com/Mathpy-257929091666547/" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i>FB Page</a>
      </span>
    
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Allen Tzeng</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 強力驅動 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主題 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.4.0</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="訪客總數">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="總瀏覽次數">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  
    
  
  <script color='255, 80, 219' opacity='1' zIndex='-1' count='100' src="/blog/lib/canvas-nest/canvas-nest-nomobile.min.js"></script>
  <script src="/blog/lib/anime.min.js?v=3.1.0"></script>
  <script src="/blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/blog/js/utils.js?v=7.4.0"></script><script src="/blog/js/motion.js?v=7.4.0"></script>
<script src="/blog/js/schemes/pisces.js?v=7.4.0"></script>

<script src="/blog/js/next-boot.js?v=7.4.0"></script>



  





  <script>
  if (CONFIG.page.isPost) {
    wpac_init = window.wpac_init || [];
    wpac_init.push({
      widget: 'Rating',
      id: 21351,
      el: 'wpac-rating',
      color: 'fc6423'
    });
    (function() {
      if ('WIDGETPACK_LOADED' in window) return;
      WIDGETPACK_LOADED = true;
      var mc = document.createElement('script');
      mc.type = 'text/javascript';
      mc.async = true;
      mc.src = '//embed.widgetpack.com/widget.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
    })();
  }
  </script>
















  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://math-py.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  function disqus_config() {
    this.page.url = "https://allen108108.github.io/blog/2019/10/07/卷積神經網路 (Convolutional Neural , CNN)/";
    this.page.identifier = "2019/10/07/卷積神經網路 (Convolutional Neural , CNN)/";
    this.page.title = '卷積神經網路 (Convolutional Neural , CNN)';};
  function loadComments() {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://math-py.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  }
    window.addEventListener('load', loadComments, false);
  
</script>

</body>
</html>
